{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-country",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-familiar",
   "metadata": {},
   "source": [
    "Load libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "shaped-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "behind-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load datasets\n",
    "def load_tweet_df(dataset_name):\n",
    "    \"\"\"\n",
    "    Load tweet dataframes (assumes filename structure)\n",
    "    \"\"\"\n",
    "    filepath_in = f'../data/derived/tweets_supervised_{dataset_name}.csv'\n",
    "    df = pd.read_csv(filepath_in)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alien-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_df = load_tweet_df('train')\n",
    "dev_df   = load_tweet_df('dev')\n",
    "test_df  = load_tweet_df('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-immunology",
   "metadata": {},
   "source": [
    "Confirm datasets have expected size and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "negative-victoria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497 records.\n",
      "1393 records.\n",
      "1396 records.\n"
     ]
    }
   ],
   "source": [
    "for df in [train_df, dev_df, test_df]:\n",
    "    print(f'{len(df)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pregnant-maker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>598502011269554176</td>\n",
       "      <td>Twitter is more likely to act on threats to co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>599285769727938560</td>\n",
       "      <td>@Claire_Medeiros i always tweet my location.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>596593444765573120</td>\n",
       "      <td>@ashleylynch me too!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "1202  598502011269554176  Twitter is more likely to act on threats to co...   \n",
       "121   599285769727938560       @Claire_Medeiros i always tweet my location.   \n",
       "1521  596593444765573120                               @ashleylynch me too!   \n",
       "\n",
       "      label  \n",
       "1202      0  \n",
       "121       0  \n",
       "1521      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unnecessary-triple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>564691340013740032</td>\n",
       "      <td>@Transition @GlennF @Spacekatgal that's the one.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>839855905902559238</td>\n",
       "      <td>RT @LIMMediaGroup: Happy #INTERNATIONALWOMENSD...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>599229500849684481</td>\n",
       "      <td>@ginmarrienne can you keep me updated? Links t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "343   564691340013740032   @Transition @GlennF @Spacekatgal that's the one.   \n",
       "1352  839855905902559238  RT @LIMMediaGroup: Happy #INTERNATIONALWOMENSD...   \n",
       "560   599229500849684481  @ginmarrienne can you keep me updated? Links t...   \n",
       "\n",
       "      label  \n",
       "343       0  \n",
       "1352      1  \n",
       "560       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinate-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>596379188635930624</td>\n",
       "      <td>RT @petfish: .@freebsdgirl I feel I should als...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>568136950227419137</td>\n",
       "      <td>NPR asked to speak to me about the wadhwa thin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>596949842661773313</td>\n",
       "      <td>@hypatiadotca you should see the shoes that @s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                                               text  \\\n",
       "57   596379188635930624  RT @petfish: .@freebsdgirl I feel I should als...   \n",
       "207  568136950227419137  NPR asked to speak to me about the wadhwa thin...   \n",
       "481  596949842661773313  @hypatiadotca you should see the shoes that @s...   \n",
       "\n",
       "     label  \n",
       "57       0  \n",
       "207      0  \n",
       "481      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "removable-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for singular value decomposition\n",
    "random_seed = 466"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-calcium",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-cloud",
   "metadata": {},
   "source": [
    "Here, I lowercase and tokenize the text with NLTK's TweetTokenizer. In reality, this step of the analysis is done as part of vectorizing the data, but I added it here manually to explore the tokens created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bizarre-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet_text(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create tokenized text column in dataframe with text lowercased and then tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # extract text\n",
    "    text = df['text']\n",
    "    # tokenize tweets and add to dataframe\n",
    "    df['tokenized_text'] = [tokenizer.tokenize(t.lower()) for t in text]\n",
    "    # reorder columns\n",
    "    df = df[['tweet_id','text','tokenized_text','label']]\n",
    "    # write to CSV\n",
    "    filepath_out = f'../data/derived/tweets_supervised_{dataset_name}_tokens.csv'\n",
    "    df.to_csv(path_or_buf=filepath_out, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "noticed-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text in datasets\n",
    "train_df = tokenize_tweet_text(train_df, 'train')\n",
    "dev_df   = tokenize_tweet_text(dev_df,   'dev')\n",
    "test_df  = tokenize_tweet_text(test_df,  'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-thickness",
   "metadata": {},
   "source": [
    "Confirm tokens match expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tamil-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573246882501148672</td>\n",
       "      <td>RT @mercurypixel: @freebsdgirl At this point.....</td>\n",
       "      <td>[rt, @mercurypixel, :, @freebsdgirl, at, this,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569200611415035904</td>\n",
       "      <td>plz stop posting pics of me that i posted a fe...</td>\n",
       "      <td>[plz, stop, posting, pics, of, me, that, i, po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>564568918321147905</td>\n",
       "      <td>Been doing things. Going to have some pretty a...</td>\n",
       "      <td>[been, doing, things, ., going, to, have, some...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>573222765957816320</td>\n",
       "      <td>hello grafana/graphite/statsd server, let's se...</td>\n",
       "      <td>[hello, grafana, /, graphite, /, statsd, serve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>564632454179213312</td>\n",
       "      <td>@sschinke @TsundereRager previous versions of ...</td>\n",
       "      <td>[@sschinke, @tsundererager, previous, versions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text  \\\n",
       "0  573246882501148672  RT @mercurypixel: @freebsdgirl At this point.....   \n",
       "1  569200611415035904  plz stop posting pics of me that i posted a fe...   \n",
       "2  564568918321147905  Been doing things. Going to have some pretty a...   \n",
       "3  573222765957816320  hello grafana/graphite/statsd server, let's se...   \n",
       "4  564632454179213312  @sschinke @TsundereRager previous versions of ...   \n",
       "\n",
       "                                      tokenized_text  label  \n",
       "0  [rt, @mercurypixel, :, @freebsdgirl, at, this,...      0  \n",
       "1  [plz, stop, posting, pics, of, me, that, i, po...      0  \n",
       "2  [been, doing, things, ., going, to, have, some...      0  \n",
       "3  [hello, grafana, /, graphite, /, statsd, serve...      0  \n",
       "4  [@sschinke, @tsundererager, previous, versions...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-lodging",
   "metadata": {},
   "source": [
    "## Bag of words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-argument",
   "metadata": {},
   "source": [
    "Here, I generate a bag of words representation of each tweet using sklearn's CountVectorizer on the tokenized tweet text. I do not remove stopwords, or remove words with a cutoff for high or low document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tribal-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_count_vectorizer(df, vectorizer_name):\n",
    "    \"\"\"\n",
    "    Train sklearn's CountVectorizer with text tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vectorizer and tokenizer\n",
    "    vectorizer = CountVectorizer(tokenizer = TweetTokenizer().tokenize)\n",
    "    vectorizer.build_tokenizer()\n",
    "    # fit vectorizer to text\n",
    "    vectorizer.fit(df['text'])\n",
    "    \n",
    "    # write to file\n",
    "    filepath_out = f'../data/derived/models/vectorizer_{vectorizer_name}.pkl'\n",
    "    pickle.dump(vectorizer, open(filepath_out, 'wb'))\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "def count_vectorize(count_vectorizer, df, vectorizer_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Use a trained CountVectorizer to transform text into count vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform data into count vectors\n",
    "    vectors = count_vectorizer.transform(df['text'])\n",
    "    \n",
    "    # save vectors to files\n",
    "    filepath_out = f'../data/derived/vectors/vector{vectorizer_name}_{dataset_name}.npz'\n",
    "    scipy.sparse.save_npz(filepath_out, vectors)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comparative-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 14740)    <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1393, 14740)    <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1396, 14740)    <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# train vectorizer\n",
    "count_vectorizer = train_count_vectorizer(train_df, 'count')\n",
    "\n",
    "# transform data into count vectors\n",
    "train_count_vectors = count_vectorize(count_vectorizer, train_df, 'count', 'train')\n",
    "dev_count_vectors   = count_vectorize(count_vectorizer, dev_df,   'count', 'dev')\n",
    "test_count_vectors  = count_vectorize(count_vectorizer, test_df,  'count', 'test')\n",
    "\n",
    "# confirm vectors have expected format and shape\n",
    "for vectors in [train_count_vectors, dev_count_vectors, test_count_vectors]:\n",
    "    print(f'{vectors.shape}    {type(vectors)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-officer",
   "metadata": {},
   "source": [
    "## TF-IDF representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-token",
   "metadata": {},
   "source": [
    "Here, I generate a term frequency-inverse document frequency representation of each tweet using sklearn's TfidfVectorizer on the tokenized tweet text. I do not remove stopwords, or remove words with a cutoff for high or low document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "substantial-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf_vectorizer(df, vectorizer_name):\n",
    "    \"\"\"\n",
    "    Train sklearn's TfidfVectorizer with text tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vectorizer and tokenizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer = TweetTokenizer().tokenize)\n",
    "    vectorizer.build_tokenizer()\n",
    "    # fit vectorizer to text\n",
    "    vectorizer.fit(df['text'])\n",
    "    \n",
    "    # write to file\n",
    "    filepath_out = f'../data/derived/models/vectorizer_{vectorizer_name}.pkl'\n",
    "    pickle.dump(vectorizer, open(filepath_out, 'wb'))\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "def tfidf_vectorize(tfidf_vectorizer, df, vectorizer_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Use a trained TfidfVectorizer to transform text into TF-IDF vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform data into TF-IDF vectors\n",
    "    vectors = tfidf_vectorizer.transform(df['text'])\n",
    "    \n",
    "    # save vectors to files\n",
    "    filepath_out = f'../data/derived/vectors/vector{vectorizer_name}_{dataset_name}.npz'\n",
    "    scipy.sparse.save_npz(filepath_out, vectors)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handy-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 14740)    <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1393, 14740)    <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1396, 14740)    <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# train vectorizer\n",
    "tfidf_vectorizer = train_tfidf_vectorizer(train_df, 'tfidf')\n",
    "\n",
    "# transform data into TF-IDF vectors\n",
    "train_tfidf_vectors = tfidf_vectorize(tfidf_vectorizer, train_df, 'tfidf', 'train')\n",
    "dev_tfidf_vectors   = tfidf_vectorize(tfidf_vectorizer, dev_df,   'tfidf', 'dev')\n",
    "test_tfidf_vectors  = tfidf_vectorize(tfidf_vectorizer, test_df,  'tfidf', 'test')\n",
    "\n",
    "# confirm vectors have expected format and shape\n",
    "for vectors in [train_tfidf_vectors, dev_tfidf_vectors, test_tfidf_vectors]:\n",
    "    print(f'{vectors.shape}    {type(vectors)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-veteran",
   "metadata": {},
   "source": [
    "## LSI representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-genesis",
   "metadata": {},
   "source": [
    "Here, I use the TF-IDF vectors as an input to singular value decomposition to create a latent semantic indexing (LSI) representation. LSI is a dimesnionality reduction technique that deals with polysemy - words that have similar meanings are combined into a single feature. I will fit LSI with three numbers of components, all fewer than the number of features in the original vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unlimited-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lsi_transformer(tfidf_vectors, n_components, random_seed):\n",
    "    \"\"\"\n",
    "    Train sklearn's TfidfVectorizer with text tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize transformer\n",
    "    transformer = TruncatedSVD(n_components = n_components, random_state = random_seed)\n",
    "    # fit transformer to TF-IDF vectors\n",
    "    transformer.fit(tfidf_vectors)\n",
    "    \n",
    "    # write to file\n",
    "    filepath_out = f'../data/derived/models/transformer_lsi{n_components}.pkl'\n",
    "    pickle.dump(transformer, open(filepath_out, 'wb'))\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sealed-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi_transform(transformer, tfidf_vectors, dataset_name):\n",
    "    \"\"\"\n",
    "    Use a trained TfidfVectorizer to transform text into TF-IDF vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform data into TF-IDF vectors\n",
    "    lsi_vectors = transformer.transform(tfidf_vectors)\n",
    "\n",
    "    # save vectors to files\n",
    "    filepath_out = f'../data/derived/vectors/vectorlsi{transformer.n_components}_{dataset_name}.csv'\n",
    "    np.savetxt(filepath_out, lsi_vectors, delimiter=',')\n",
    "    \n",
    "    return lsi_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ahead-recording",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 5)    <class 'numpy.ndarray'>\n",
      "(1393, 5)    <class 'numpy.ndarray'>\n",
      "(1396, 5)    <class 'numpy.ndarray'>\n",
      "(6497, 10)    <class 'numpy.ndarray'>\n",
      "(1393, 10)    <class 'numpy.ndarray'>\n",
      "(1396, 10)    <class 'numpy.ndarray'>\n",
      "(6497, 50)    <class 'numpy.ndarray'>\n",
      "(1393, 50)    <class 'numpy.ndarray'>\n",
      "(1396, 50)    <class 'numpy.ndarray'>\n",
      "(6497, 100)    <class 'numpy.ndarray'>\n",
      "(1393, 100)    <class 'numpy.ndarray'>\n",
      "(1396, 100)    <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# iterate over components hyperparameter\n",
    "for n_components in [5, 10, 50, 100]:\n",
    "    \n",
    "    # train transformer\n",
    "    lsi_transformer = train_lsi_transformer(train_tfidf_vectors, n_components, random_seed)\n",
    "\n",
    "    # transform data into TF-IDF vectors\n",
    "    train_lsi_vectors = lsi_transform(lsi_transformer, train_tfidf_vectors, 'train')\n",
    "    dev_lsi_vectors   = lsi_transform(lsi_transformer, dev_tfidf_vectors,   'dev')\n",
    "    test_lsi_vectors  = lsi_transform(lsi_transformer, test_tfidf_vectors,  'test')\n",
    "\n",
    "    # confirm vectors have expected format and shape\n",
    "    for vectors in [train_lsi_vectors, dev_lsi_vectors, test_lsi_vectors]:\n",
    "        print(f'{vectors.shape}    {type(vectors)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
