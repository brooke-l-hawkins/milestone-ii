{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-country",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-familiar",
   "metadata": {},
   "source": [
    "Load libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "shaped-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "behind-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filepath_in = f'../data/derived/tweets_unsupervised.csv'\n",
    "tweet_df = pd.read_csv(filepath_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-construction",
   "metadata": {},
   "source": [
    "Note: Much of this code is repurposed from the train_supervised_data.ipynb file, with minor changes for file names, and the lack of split train/dev/test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-dayton",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-nudist",
   "metadata": {},
   "source": [
    "Here, I lowercase and tokenize the text with NLTK's TweetTokenizer. In reality, this step of the analysis is done as part of vectorizing the data, but I added it here manually to explore the tokens created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflected-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet_text(df):\n",
    "    \"\"\"\n",
    "    Create tokenized text column in dataframe with text lowercased and then tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # extract text\n",
    "    text = df['text']\n",
    "    # tokenize tweets and add to dataframe\n",
    "    df['tokenized_text'] = [tokenizer.tokenize(t.lower()) for t in text]\n",
    "    # reorder columns\n",
    "    df = df[['tweet_id','text','tokenized_text','label']]\n",
    "    # write to CSV\n",
    "    filepath_out = f'../data/derived/tweets_unsupervised_tokens.csv'\n",
    "    df.to_csv(path_or_buf=filepath_out, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fluid-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text in datasets\n",
    "tweet_df = tokenize_tweet_text(tweet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-version",
   "metadata": {},
   "source": [
    "Confirm tokens match expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "champion-deadline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>575742743730745344</td>\n",
       "      <td>@HPluckrose @Feminazi_Front men and women have...</td>\n",
       "      <td>[@hpluckrose, @feminazi_front, men, and, women...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>575417042150387712</td>\n",
       "      <td>@nat_com1 @Feminazi_Front We know there is a b...</td>\n",
       "      <td>[@nat_com1, @feminazi_front, we, know, there, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603682460191203328</td>\n",
       "      <td>@ShaePhoenix Would looooove to see a feminazi ...</td>\n",
       "      <td>[@shaephoenix, would, looooove, to, see, a, fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570994652993556481</td>\n",
       "      <td>RT @PeerWorker: @freebsdgirl You just lost $10...</td>\n",
       "      <td>[rt, @peerworker, :, @freebsdgirl, you, just, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576554873468035072</td>\n",
       "      <td>@eugenegaytard BUT BUT WOMEN CAN\"T RAPE! no th...</td>\n",
       "      <td>[@eugenegaytard, but, but, women, can, \", t, r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text  \\\n",
       "0  575742743730745344  @HPluckrose @Feminazi_Front men and women have...   \n",
       "1  575417042150387712  @nat_com1 @Feminazi_Front We know there is a b...   \n",
       "2  603682460191203328  @ShaePhoenix Would looooove to see a feminazi ...   \n",
       "3  570994652993556481  RT @PeerWorker: @freebsdgirl You just lost $10...   \n",
       "4  576554873468035072  @eugenegaytard BUT BUT WOMEN CAN\"T RAPE! no th...   \n",
       "\n",
       "                                      tokenized_text  label  \n",
       "0  [@hpluckrose, @feminazi_front, men, and, women...      0  \n",
       "1  [@nat_com1, @feminazi_front, we, know, there, ...      0  \n",
       "2  [@shaephoenix, would, looooove, to, see, a, fe...      0  \n",
       "3  [rt, @peerworker, :, @freebsdgirl, you, just, ...      0  \n",
       "4  [@eugenegaytard, but, but, women, can, \", t, r...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview dataframe\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-hacker",
   "metadata": {},
   "source": [
    "## Bag of words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-atlanta",
   "metadata": {},
   "source": [
    "Here, I generate a bag of words representation of each tweet using sklearn's CountVectorizer on the tokenized tweet text. I do not remove stopwords, or remove words with a cutoff for high or low document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "offshore-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_count_vectorizer(df, vectorizer_name):\n",
    "    \"\"\"\n",
    "    Train sklearn's CountVectorizer with text tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vectorizer and tokenizer\n",
    "    vectorizer = CountVectorizer(tokenizer = TweetTokenizer().tokenize)\n",
    "    vectorizer.build_tokenizer()\n",
    "    # fit vectorizer to text\n",
    "    vectorizer.fit(df['text'])\n",
    "    \n",
    "    # write to file\n",
    "    filepath_out = f'../data/derived/models/unsupervised_vectorizer_{vectorizer_name}.pkl'\n",
    "    pickle.dump(vectorizer, open(filepath_out, 'wb'))\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "def count_vectorize(count_vectorizer, df, vectorizer_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Use a trained CountVectorizer to transform text into count vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform data into count vectors\n",
    "    vectors = count_vectorizer.transform(df['text'])\n",
    "    \n",
    "    # save vectors to files\n",
    "    filepath_out = f'../data/derived/vectors/vectorcount_{dataset_name}.npz'\n",
    "    scipy.sparse.save_npz(filepath_out, vectors)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "israeli-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3839, 10536)    <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# train vectorizer\n",
    "count_vectorizer = train_count_vectorizer(tweet_df, 'count')\n",
    "\n",
    "# transform data into count vectors\n",
    "count_vectors = count_vectorize(count_vectorizer, tweet_df, 'count', 'unsupervised')\n",
    "\n",
    "# confirm vectors have expected format and shape\n",
    "print(f'{count_vectors.shape}    {type(count_vectors)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-rolling",
   "metadata": {},
   "source": [
    "## TF-IDF representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-screw",
   "metadata": {},
   "source": [
    "Here, I generate a term frequency-inverse document frequency representation of each tweet using sklearn's TfidfVectorizer on the tokenized tweet text. I do not remove stopwords, or remove words with a cutoff for high or low document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "upset-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf_vectorizer(df, vectorizer_name):\n",
    "    \"\"\"\n",
    "    Train sklearn's TfidfVectorizer with text tokenized by NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vectorizer and tokenizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer = TweetTokenizer().tokenize)\n",
    "    vectorizer.build_tokenizer()\n",
    "    # fit vectorizer to text\n",
    "    vectorizer.fit(df['text'])\n",
    "    \n",
    "    # write to file\n",
    "    filepath_out = f'../data/derived/models/unsupervised_vectorizer_{vectorizer_name}.pkl'\n",
    "    pickle.dump(vectorizer, open(filepath_out, 'wb'))\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "def tfidf_vectorize(tfidf_vectorizer, df, vectorizer_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Use a trained TfidfVectorizer to transform text into TF-IDF vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform data into TF-IDF vectors\n",
    "    vectors = tfidf_vectorizer.transform(df['text'])\n",
    "    \n",
    "    # save vectors to files\n",
    "    filepath_out = f'../data/derived/vectors/vector{vectorizer_name}_{dataset_name}.npz'\n",
    "    scipy.sparse.save_npz(filepath_out, vectors)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-glasgow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3839, 10536)    <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# train vectorizer\n",
    "tfidf_vectorizer = train_tfidf_vectorizer(tweet_df, 'tfidf')\n",
    "\n",
    "# transform data into count vectors\n",
    "tfidf_vectors = tfidf_vectorize(tfidf_vectorizer, tweet_df, 'tfidf', 'unsupervised')\n",
    "\n",
    "# confirm vectors have expected format and shape\n",
    "print(f'{tfidf_vectors.shape}    {type(tfidf_vectors)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
